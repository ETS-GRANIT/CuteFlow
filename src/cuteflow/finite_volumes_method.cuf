module finite_volumes_method
  implicit none
contains
  subroutine solve_finite_volumes
    use cudafor
    use mpi
    use precision_kind
    use global_data
    use global_data_device
    use file_id
    use pre_post_traitement
    use flux_riemann
    use cfl_condition
    use source_terms
    use memory_exchange_functions
    use maillage

    implicit none

    !*** Variables locales ***
    integer :: stat(MPI_STATUS_SIZE)
    integer :: iter_nodes, iter_entree
    integer :: i, j, k

    real(fp_kind)   :: time_start, time_stop
    integer         :: ierr
    type(cudaEvent) :: startEvent , stopEvent

    tc          = 0.d0
    t_reg_perm  = 0.d0

    nt             = 0 
    reg_perm_reached  = .FALSE.

    allocate(resc_d(ndln,nelt), res_d(ndln,nelt))

    resc_d = 0.d0
    res_d = 0.d0

    allocate(sourcfric_d(ndln,nelt))
    allocate(sourcbath_d(ndln,nelt))
    allocate(io_identifier(nelt), io_identifier_d(nelt))

    allocate(afm1_d(nelt, 9))

    allocate(deltatmin_d(nelt-nelt_fant_recep))

    if (ndi>0) allocate(debit_entree_arr(ndi-1,2), debit_entree_arr_d(ndi-1,2))
    if (ndo>0) allocate(debit_sortie_arr(ndo-1),debit_sortie_arr_d(ndo-1))

    call get_io_index

    if(num_mpi_process > 1 .and. is_cgns==1) then
      nelt_send_tot = 0
      nelt_nonsend = nelt
      do i=1,nelt_fant_envoi_bloc
        nelt_send_tot = nelt_send_tot + elt_fant_envoi_list(i)%length
        do j=1,elt_fant_envoi_list(i)%length
          nelt_nonsend = min(nelt_nonsend, elt_fant_envoi_list(i)%elem_id(j))
        end do
      end do

      nelt_send_tot_d = nelt_send_tot
      nelt_nonsend_d = nelt_nonsend

      allocate(elt_fant_envoi_list_glob(nelt_send_tot))
      allocate(elt_fant_envoi_list_glob_d(nelt_send_tot))

      k=1
      do i=1,nelt_fant_envoi_bloc
        do j=1,elt_fant_envoi_list(i)%length
          elt_fant_envoi_list_glob(k) = elt_fant_envoi_list(i)%elem_id(j)
          k=k+1
        end do
      end do

      elt_fant_envoi_list_glob_d = elt_fant_envoi_list_glob
    end if

    tblock = dim3(32, 1, 1)
    grid   = dim3(ceiling(real(nelt)/real(tblock%x)), 1, 1)
    grid_halved   = dim3(ceiling(real(nelt)/real(2*tblock%x)), 1, 1)

    tblock_reduc = dim3(1024, 1, 1)
    grid_reduc   = dim3(ceiling(real(nelt)/real(tblock_reduc%x)), 1, 1)

    block_copy = dim3(32,1,1) 
    grid_copy = dim3(ceiling(real(nelt)/real(block_copy%x)),1,1)

    tblock_3y = dim3(8, ns, 1)
    grid_3y   = dim3(ceiling(real(nelt)/real(tblock_3y%x)), 1, 1)

    tblock_usc = dim3(32, 1, 1)
    grid_usc   = dim3(ceiling(real(nmax_ptset)/real(tblock_usc%x)), nelt_fant_envoi_bloc, 1)

    tblock_send = dim3(32, 1, 1)
    grid_send   = dim3(ceiling(real(nelt_send_tot)/real(tblock_send%x)), 1, 1)

    tblock_nonsend = dim3(32, 1, 1)
    grid_nonsend   = dim3(ceiling(real(nelt_nonsend)/real(tblock_nonsend%x)), 1, 1)

    tblock_3y_send = dim3(8, ns, 1)
    grid_3y_send   = dim3(ceiling(real(nelt_send_tot)/real(tblock_3y_send%x)), 1, 1)

    tblock_3y_nonsend = dim3(8, ns, 1)
    grid_3y_nonsend   = dim3(ceiling(real(nelt_nonsend)/real(tblock_3y_nonsend%x)), 1, 1)

    tblock_surf_entree = dim3(32, 1, 1)
    grid_surf_entree   = dim3(ceiling(real(nelt_input)/real(tblock_surf_entree%x)), 1, 1)


    !!Initial dt exchange
    call cflcond<<<grid, tblock>>>(vdlg0_d,deltatmin_d)
    call reduction<<<grid_reduc, tblock_reduc, tblock_reduc%x*sizeof(deltatmin_d(1))>>>(deltatmin_d)
    if(num_mpi_process>1) call MPI_ALLREDUCE(MPI_IN_PLACE,dt_d,1,fp_kind_mpi,MPI_MIN,MPI_COMM_WORLD, mpi_ierr)
    ierr = cudaMemcpy(dt,dt_d,1)

    ierr = cudaEventCreate(startEvent)
    ierr = cudaEventCreate(stopEvent)
    ierr = cudaEventRecord(startEvent,0)

    write(*,*) 'Loop over time starts' 
    do while( (tc - ts + 2*tol ) < tol .and. reg_perm_reached==.FALSE.)
      is_solnode = .FALSE.

      nt = nt + 1

      ierr = cudaDeviceSynchronize()

      debit_entree  = 0.d0
      debit_sortie  = 0.d0

      if(iupwind==3) call update_sol_nodes<<<grid_nodes, tblock_nodes, 0, stream1>>> 

      if(inlet=="phys_inflow" .and. nombre_entrees>0) then
          ierr = cudamemcpyasync(surf_entree_d, zero_entree_d, nombre_entrees, stream1)
          call calcul_surf_entree<<<grid_surf_entree, tblock_surf_entree, tblock_surf_entree%x*sizeof(1.0d0), stream1>>>()
          ierr =  cudastreamsynchronize(stream1)
          call mpi_allreduce(mpi_in_place,surf_entree_d,nombre_entrees,fp_kind_mpi,mpi_sum,mpi_comm_world, mpi_ierr)
          ! surf_entree = surf_entree_d
          ! do i=1,nombre_entrees
          !   print*,i, surf_entree(i)
          ! end do
      end if

      !If mesh large enough, optimise computations and communications overlapp
      if(is_cgns==1 .and. (nelt>20000 .and. num_mpi_process>1)) then
      ! if(.FALSE.) then
      ! if(.TRUE.) then
        !!Phase 1 computation on the send cells only
        only_send_cells=.TRUE.
        only_send_cells_d=only_send_cells
        call compute_flux<<<grid_3y_send, tblock_3y_send, ns*tblock_3y_send%x*sizeof(1.0d0), stream1>>>()
        call compute_source_terms<<<grid_send, tblock_send, 0, stream2>>>()
        ierr = cudaStreamSynchronize(stream1)
        call update_sol_with_flux_and_source<<<grid_send, tblock_send, 0, stream2>>>()

        !!Memory exchange is started
        if(num_mpi_process>1 .and. is_cgns==1) then
          call update_send_cells<<<grid_usc, tblock_usc, 0, stream2>>>()
        end if

        !!  Echange sur GPU directement bloc
        if(num_mpi_process>1) then
          if(is_cgns) then
            ierr = cudastreamsynchronize(stream2)
            call gpu_echange_cgns(vdlg_d, ndln)
          else
            call gpu_echange_fant_bloc_async(vdlg_d, ndln)
          end if
        end if

        !!Phase 2 computation on the cells that are not send cells
        only_send_cells=.FALSE.
        only_send_cells_d=only_send_cells
        call compute_flux<<<grid_3y_nonsend, tblock_3y_nonsend, ns*tblock_3y_nonsend%x*sizeof(1.0d0), stream1>>>()
        call compute_source_terms<<<grid_nonsend, tblock_nonsend, 0, stream2>>>()
        ierr = cudaStreamSynchronize(stream1)
        call update_sol_with_flux_and_source<<<grid_nonsend, tblock_nonsend, 0, stream2>>>()

      else
        only_send_cells=.FALSE.
        only_send_cells_d=only_send_cells
        call compute_flux<<<grid_3y, tblock_3y, ns*tblock_3y%x*sizeof(1.0d0), stream1>>>()
        call compute_source_terms<<<grid, tblock, 0, stream2>>>()
        ierr = cudaStreamSynchronize(stream1)
        call update_sol_with_flux_and_source<<<grid, tblock, 0, stream2>>>()

        !!Memory exchange is started
        if(num_mpi_process>1 .and. is_cgns==1) then
          call update_send_cells<<<grid_usc, tblock_usc, 0, stream2>>>()
        end if

        !!  Echange sur GPU directement bloc
        if(num_mpi_process>1) then
          if(is_cgns) then
            ierr = cudastreamsynchronize(stream2)
            call gpu_echange_cgns(vdlg_d, ndln)
          else
            call gpu_echange_fant_bloc_async(vdlg_d, ndln)
          end if
        end if
      end if

      !Compute outflow
      if(ndo>1) then
        ierr = cudaMemcpyAsync(debit_sortie_arr,debit_sortie_arr_d,size(debit_sortie_arr),stream1)
        ierr = cudaStreamSynchronize(stream1)
        debit_sortie  =  debit_sortie + sum(debit_sortie_arr)
      end if

      !Compute inflow
      if(ndi>1) then
        ierr = cudaMemcpyAsync(debit_entree_arr,debit_entree_arr_d,size(debit_entree_arr),stream1)
        ierr = cudaStreamSynchronize(stream1)
        !En fait on parcours les arretes numérote dans get_io_index
        do iter_nodes=1,ndi-1
          !!iter_entree = numero_ndinput(iter_nodes)
          iter_entree = debit_entree_arr(iter_nodes,2)
          debit_entree(iter_entree) = debit_entree(iter_entree) + debit_entree_arr(iter_nodes,1)
        end do
      end if

      !!Reduce sum of all debits
      if(num_mpi_process>1) then
        call MPI_ALLREDUCE(MPI_IN_PLACE,debit_entree,nombre_entrees,fp_kind_mpi,MPI_SUM,MPI_COMM_WORLD, mpi_ierr)
        call MPI_ALLREDUCE(MPI_IN_PLACE,debit_sortie,1,fp_kind_mpi,MPI_SUM,MPI_COMM_WORLD, mpi_ierr)
      end if

      call compute_time_step(stream2)

      !!Attendre fin de l'échange mémoire de vdlg
      if(num_mpi_process>1) then
        call MPI_WAITALL(size(reqsend),reqsend,sendstat,mpi_ierr)
        call MPI_WAITALL(size(reqrecv),reqrecv,recvstat,mpi_ierr)
      end if

      !!Test to vertex and back
      ! ierr = cudaDeviceSynchronize()
      ! call MPI_BARRIER(MPI_COMM_WORLD,mpi_ierr)
      ! ierr = cudaDeviceSynchronize()
      ! call update_sol_nodes<<<grid_nodes, tblock_nodes>>> 
      ! ierr = cudaDeviceSynchronize()
      ! call update_sol_elems<<<grid, tblock>>> 

      ierr = cudaMemcpyAsync(vdlg0_d, vdlg_d, ndln*nelt, stream1)

      tc = tc + dt

      !!Verifie la condition de régime stationnaire
      call check_steady_state

      !!Affiche l'avancement à l'écran
      call print_status_in_console

      !!Sauvegarde la solution de différentes manières si requis
      call global_save_sol_visu
    enddo 

    write(*,*) 'Loop on time ends at iteration : ', nt

    ierr = cudaEventRecord (stopEvent,0)
    ierr = cudaEventSynchronize(stopEvent)
    ierr = cudaEventElapsedTime(time_cuda, startEvent, stopEvent)

    vdlg = vdlg_d

    if(reg_perm_reached) tc = t_reg_perm 

    if(solrestart) call save_sol_for_restart

    if(sortie_finale_bluekenue) call bluekenue_export

  end subroutine solve_finite_volumes

  attributes(global) subroutine update_sol_with_flux_and_source
    use precision_kind
    use global_data_device

    implicit none

    integer :: ti, gi, ggi,nelt_limit
    real(fp_kind) :: resini(3)

    ti = threadIdx%x

    if(only_send_cells_d) then
      nelt_limit = nelt_send_tot_d
      ggi = (blockIdx%x - 1)*blockDim%x + ti
      if(ggi<=nelt_limit) then
        gi = elt_fant_envoi_list_glob_d(ggi)
      end if
    else
      nelt_limit = nelt_d-nelt_fant_recep_d
      gi = (blockIdx%x - 1)*blockDim%x + ti
      ggi=gi
    end if

    if(ggi <= nelt_limit) then
      if ( friction_d == 1) then
        if ( fricimplic_d == 1 .or. fricimplic_d == 2 ) then
          resini = ( sourcbath_d(:,gi) + sourcfric_d(:,gi) - resc_d(:,gi) ) / surf_d(gi)
          res_d(1,gi) = sum(afm1_d(gi,1:3)*resini)
          res_d(2,gi) = sum(afm1_d(gi,4:6)*resini)
          res_d(3,gi) = sum(afm1_d(gi,7:9)*resini)
        else
          res_d(:,gi) = (sourcbath_d(:,gi)+ sourcfric_d(:,gi) - resc_d(:,gi)) / surf_d(gi)
        endif
      else 
        res_d(:,gi) = (sourcbath_d(:,gi) - resc_d(:,gi)) / surf_d(gi)
      endif

      vdlg_d(:,gi) = vdlg0_d(:,gi) + dt_d * res_d(:,gi)

      if(vdlg_d(1,gi) <= tolisec_d) then
        ! vdlg_d(1,gi) = 0.
        vdlg_d(1,gi) = tolisec_d
        vdlg_d(2,gi) = 0.
        vdlg_d(3,gi) = 0.
      end if
    endif
  end subroutine update_sol_with_flux_and_source

  subroutine get_io_index
    use precision_kind
    use global_data
    use global_data_device
    implicit none
    integer :: i, j, k, counter

    if(ndi>0) then
      debit_entree_arr = 0.d0
      debit_entree_arr_d = debit_entree_arr
    endif
    if(ndo>0) then
      debit_sortie_arr = 0.d0
      debit_sortie_arr_d = debit_sortie_arr
    endif

    io_identifier = 0

    counter = 1
    do i = 1, nelt
      if(boundary(i,1,1) == -1 .or. boundary(i,2,1) == -1 .or. boundary(i,3,1) == -1) then 
        io_identifier(i) =  counter
        counter = counter + 1
      endif
    enddo

    counter = 1
    do i = 1, nelt
      if(boundary(i,1,1) == -2 .or. boundary(i,2,1) == -2 .or. boundary(i,3,1) == -2) then 
        io_identifier(i) =  counter
        counter = counter + 1
      endif
    enddo

    io_identifier_d = io_identifier
    call cuda_glerror('get io index',1)
  end subroutine get_io_index
  
  subroutine print_status_in_console 
    use precision_kind
    use global_data
    implicit none

    integer :: iter_entree

    ! *** Affichage de l'avancement ***
    if ( mod(nt,freqaffich)==0 ) then
      print*,'-----------------------------------------------'

      write(*,*) 'Iteration number ', nt, ", t = ", tc
      ! if(solvisu>0) print*,'Next solution time save = ', tsolvisu(cptsolvisu), cptsolvisu

      do iter_entree=1,nombre_entrees
        print*, 'debit_entree ', iter_entree, " = " ,debit_entree(iter_entree)
      end do

      print*, 'debit_entree_sum =', debit_entree_sum, 'M3/S'  
      print*, 'debit_sortie     =', debit_sortie, 'M3/S'  

      print*,'-----------------------------------------------'
    endif

  end subroutine print_status_in_console 


  subroutine check_steady_state
    use precision_kind
    use global_data
    implicit none

    integer :: iter_entree

    debit_entree_sum = sum(debit_entree)

    if ( debit_entree_sum > 1e-8 .and. abs(debit_entree_sum - debit_sortie)/debit_entree_sum < tol_reg_perm ) then
      iter_reg_perm = iter_reg_perm + 1
      if( iter_reg_perm > 1000) then
        print*,'-----------------------------------------------'
        print*, 'LE REGIME PERMANENT EST ATTEINT'
        print*,'-----------------------------------------------'
        t_reg_perm = tc

        reg_perm_reached = .TRUE.
      end if
    else
      iter_reg_perm = 0
    endif

  end subroutine check_steady_state
end module finite_volumes_method
