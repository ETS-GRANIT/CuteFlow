program main
  ! ======================================================================
  !   CUTEFLOW : programme de simulation par volumes finis / solveurs de Riemann
  !               des ecoulements de canalisations, bris de barrages et innondations.
  !               Equations de base : Shallow Water Equations
  ! ======================================================================
  !     Auteur : Azzeddine Soulaimani & Youssef Loukili  & Jean-Marie Zokagoa & 
  !              Vincent Delmas GRANIT ETSMTL
  !     version : 4.0  ;  December 2019
  ! ======================================================================

  use mpi

  use precision_kind
  use global_data
  use file_id
  use cudafor
  use data_transfer
  use global_data_device
  use mpi_select_gpu
  use pre_post_traitement
  use finite_volumes_method
  use initial_condition
  use maillage

  implicit none

  integer         :: i, ii, j, p, kk, ss, ie, iel, nt, n_MC, i_MC, af_input
  integer         :: nsbloc, nreste, tot
  character(4)    :: i_MC_char
  character(8)    :: ii_str
  integer         :: status, l, ie_coupe_transv, ierr, dev
  real(fp_kind)   :: d2, t2, dtime2, tcm2, d, t, time_start, time_stop
  real(fp_kind)   :: errf, errf1, errf2, errf3, errf4, errf5
  type(dim3 )     :: grid, tblock, grid_nodes, tblock_nodes, grid11, tblock11
  type(cudaEvent) :: startEvent , stopEvent
  integer                     :: stat(MPI_STATUS_SIZE)

  call MPI_INIT(mpi_ierr)
  call MPI_COMM_SIZE (MPI_COMM_WORLD, num_mpi_process, mpi_ierr)
  call MPI_COMM_RANK (MPI_COMM_WORLD, mpi_process_id, mpi_ierr)
  write(mpi_process_id_string, "(I3)") mpi_process_id

  !! Setting proper GPU device for each MPI process (cf mpi_select_gpu.cuf)
  call assignDevice(dev)
  ierr = cudaSetDevice(dev)
  ierr = cudaGetDeviceProperties (prop , 0)
  write(*,"('Device name:',a, 'thread id : ', i1)") trim(prop%name), mpi_process_id
  write(*,"('Compute capability : ',i1,'.',i1)") prop%major, prop%minor

  !!Création des cuda stream
  istat = cudaStreamCreate(stream1)
  istat = cudaStreamCreate(stream2)
  istat = cudaStreamCreate(stream3)

  !! Lecture du fichier "donnees.f"
  call lec_donnees

  allocate(dt,dt_max)
  dt_max=1000000.0D0

  !! Initization of the iteration counters
  cptsolvisu       = 1
  cptsolrestart    = 1
  cptsolsimple     = 1
  ie_coupe_transv  = 0
  tc_init          = 0

  !! Data transfer from host to device
  call cpu_to_gpu_initial_data_transfer

  !! Ouverture/creation des fichiers (opening / creating files)
  call ouv_fichiers

  !! Construction / lecture  et  stockage du maillage 
  call lecture_maillage

  !! Creation du block et grille pour le calcul sur GPU
  tblock = dim3(64, 1, 1)
  grid   = dim3(ceiling(real(nelt)/real(tblock%x)), 1, 1)

  call set_undefined_boudaries_as_walls<<<grid, tblock>>>
  call cuda_glerror('set_undefined_boundaries_as_walls',1)

  !!Allocation des requettes send recv pour les echanges mpi non bloquant
  if(is_cgns) then
    allocate(reqsend(nelt_fant_envoi_bloc), sendstat(MPI_STATUS_SIZE,nelt_fant_envoi_bloc))
    allocate(reqrecv(nelt_fant_recep_bloc), recvstat(MPI_STATUS_SIZE,nelt_fant_recep_bloc))
  else
    tot=0
    do j=1,nelt_fant_envoi_bloc
      nsbloc = ndln*elt_fant_envoi_bloc(j,2)/768 + 1
      nreste = ndln*elt_fant_envoi_bloc(j,2)-(nsbloc-1)*768
      tot=tot+nsbloc
    end do
    allocate(reqsend(tot), sendstat(MPI_STATUS_SIZE,tot))

    tot=0
    do j=1,nelt_fant_recep_bloc
      nsbloc = ndln*elt_fant_recep_bloc(j,2)/768 + 1
      nreste = ndln*elt_fant_recep_bloc(j,2)-(nsbloc-1)*768
      tot=tot+nsbloc
    end do
    allocate(reqrecv(tot), recvstat(MPI_STATUS_SIZE,tot))
  end if

  !! Allocation des matrices et vecteurs
  allocate( surf(nelt), zm(nelt) )
  allocate( surf_d(nelt), zm_d(nelt) )

  allocate( vdlg(ndln,nelt), vdlg0(ndln,nelt))
  allocate( vdlg_d(ndln,nelt), vdlg0_d(ndln,nelt))
  allocate( vdlg_visu(nelt,8))

  allocate( vdlg_nodes(ndln,nnt))
  allocate( vdlg_nodes_d(ndln,nnt))

  allocate( tsoljauge(200000), tsolrestart(200000), tsolsimple(200000))

  if(visu_snapshots==1) visu_snapshots=2
  allocate(tsolvisu(visu_snapshots), solname_visu(visu_snapshots))

  !! Construction of visualization vectors in post-processing
  if (solvisu > 0) then
    do ii = 1,visu_snapshots
      write(ii_str,"(I8)") ii-1
      solname_visu(ii) = "Solution_"//trim(adjustl(trim(ii_str)))
      tsolvisu(ii) = (ii-1)*ts/real(visu_snapshots-1)
    enddo
  endif

  if (solsimple /= 0) then
    do ii = 1,simple_snapshots
      tsolsimple(ii) = (ii-1)*ts/(1.0*simple_snapshots)
    enddo
  endif

  if (solrestart == 1) then
    do ii = 1,restart_snapshots
      tsolrestart(ii)= (ii-1)*ts/(1.0*restart_snapshots)
    enddo
  endif 

  call compute_surface_and_bathy<<<grid, tblock>>>
  call cuda_glerror('compute_surface_and_bahty',1)

  zm      = zm_d
  surf    = surf_d

  write(*,*) '======================================================================'
  write(*,*) ' Parameters'
  write(*,*) '======================================================================'
  write(*,*) 'meshfile            = ', meshfile 
  write(*,*) 'cotemin             = ', cotemin
  write(*,*) 'elt_bound           = ', elt_bound
  write(*,*) 'H_AMONT             = ', H_AMONT 
  write(*,*) 'U_AMONT             = ', U_AMONT
  write(*,*) 'V_AMONT             = ', V_AMONT
  write(*,*) 'H_AVAL              = ', H_AVAL
  write(*,*) 'U_AVAL              = ', U_AVAL
  write(*,*) 'V_AVAL              = ', V_AVAL 
  write(*,*) 'iflux               =', iflux
  write(*,*) 'Fricimplic          =', fricimplic
  write(*,*) 'solrestart          =', solrestart
  write(*,*) 'restart_snapshots   =', restart_snapshots
  write(*,*) 'visu_snapshots     =', visu_snapshots
  write(*,*) 'tol_reg_perm        =', tol_reg_perm
  write(*,*) 'tol                 =', tol
  write(*,*) 'tolisec             =', tolisec
  write(*,*) 'tolaffiche          =', tolaffiche
  if(mpi_process_id==0) write(*,*) "nombre d'elements totaux = ",nelt_global_non_fant
  write(*,*) 'freqaffich          =', freqaffich
  write(*,*) 'nombre entree       =', nombre_entrees
  do af_input=1,nombre_entrees
    write(*,*) 'debitglob       ', af_input, " =", debitglob(af_input)
    write(*,*) 'longueur entree ', af_input, " =", long_entree(af_input)
  end do
  write(*,*) 'nombre sortie       =', nombre_sorties
  do af_input=1,nombre_sorties
    write(*,*) 'H_SORTIE        ', af_input, " =", h_sortie(af_input)
  end do
  write(*,*) '======================================================================'
  write(*,*) '======================================================================'

  !! Application des conditions initiales
  if (solinit==1) then
    call read_initial_condition
  else
    call set_initial_condition<<<grid, tblock>>>
    call cuda_glerror('set_initial_condition',1)
  endif               

  vdlg0 = vdlg0_d
  vdlg = vdlg0
  vdlg_d = vdlg

  !! Calcul de la solution initiale sur les noeuds
  ! call sol_nodes(sh0,she0,shu0,shv0,su0,sv0)

  !!  Storage for postraitement with possibility of animation (Tecplot)
  if(solvisu>0) then             
    start=1
    if(solvisu==1 .or. solvisu==3) then
      call paraview_export(cptsolvisu)
    end if
    !CGNS EXPORT
    if(is_cgns==1 .and. (solvisu==2 .or. solvisu==3)) then
      call cgns_export(cptsolvisu)
    end if
    cptsolvisu = cptsolvisu + 1
  endif

  if(solsimple/=0) then
    start=1
    call simple_export(cptsolsimple)
    cptsolsimple = cptsolsimple + 1
  end if

  !! Initialisation du temps et calcul de dt via la CFL
  tc = 0.00

  !! Résolution par méthode volumes finis
  call solve_finite_volumes

  print*, '==========================================================================='
  print*, '=========================   FIN DE LA SIMULATION   ========================'
  print*, '==========================================================================='
  print*, ''
  print*, 'DUREE DU CALCUL :', time_cuda, ' Secondes '
  if (time_cuda >= 60)    print*, '   soit:',  time_cuda/60,   ' Minutes'
  if (time_cuda >= 3600)  print*, '   soit:', time_cuda/3600,  ' Heures'
  if (time_cuda >= 86400) print*, '   soit:', time_cuda/86400, ' Jours'
  print*, '==========================================================================='
  print*, '==========================================================================='   

  call MPI_FINALIZE(mpi_ierr)
end program main
